Martin Escardo, Paulo Oliva, 2-27 July 2021

We study finite, history dependent games of perfect information using
selection functions and dependent-type trees.

This is based on our previous work.

  [1] M.H. EscardÃ³ and Paulo Oliva. Sequential Games and Optimal
      Strategies. Proceedings of the Royal Society A, 467:1519-1545,
      2011. https://doi.org/10.1098/rspa.2010.0471

We generalize [1] by considering the case that the type Xâ‚– of moves xâ‚–
at round k depends on the moves played at the previous rounds. So in a
sequence of moves xâ‚€,xâ‚,xâ‚‚,â€¦, we have that

  * the type Xâ‚€ of initial moves doesn't depend on anything,
  * the type Xâ‚ depends on the first move xâ‚€ : Xâ‚€,
  * the type Xâ‚‚ depends on the first move xâ‚€ : Xâ‚€ and the
    second move xâ‚ : Xâ‚.
  â€¦

In order to achieve this generalization, we work with game trees
whose nodes are labelled by types that collect the allowed moves at a
given round. Then a play xâ‚€,xâ‚,xâ‚‚,â€¦, is a path in such a tree.

This formulation of the notion of game naturally accounts for finite
games of *unbounded* length, which in [1] was achieved by continuous,
infinite games instead.
\begin{code}

{-# OPTIONS --without-K --safe --auto-inline #-} -- --exact-split

open import MLTT.Spartan hiding (J)
open import UF.Base
open import UF.FunExt

open import TypeTopology.SigmaDiscreteAndTotallySeparated

module Games.FiniteHistoryDependent (fe : Fun-Ext) where

\end{code}

We represent the moves of a history-dependent sequential game by a
dependent-type tree, collected in a type ğ•‹.  This is either an empty
tree [] or else has a type X of initial moves at the root, and,
inductively, a family Xf of subtrees indexed by elements of X, which
is written X âˆ· Xf. We refer to the family Xf as a forest. We let Xt
range over such trees.

 * Xt ranges over dependent-type trees.
 * Xf ranges over dependent-type forests.

\begin{code}

data ğ•‹ : Typeâ‚ where
  []  : ğ•‹
  _âˆ·_ : (X : Type) (Xf : X â†’ ğ•‹) â†’ ğ•‹

\end{code}

The type of full paths in a tree Xt, from the root to a leaf, is
inductively defined as follows:

\begin{code}

Path : ğ•‹ â†’ Type
Path []       = ğŸ™
Path (X âˆ· Xf) = Î£ x ê‰ X , Path (Xf x)

\end{code}

As discussed above, a play in a game is defined to be such a path.

The idea is that we choose a move x, and then, inductively, a path in
the subtree Xf x.

The variable xs ranges over paths, that is, elements of the type
Path Xt for a dependent-type-tree Xt.

\begin{code}

pattern âŸ¨âŸ©        = â‹†
pattern _::_ x xs = (x , xs)

path-head : {X : Type} {Xf : X â†’ ğ•‹} â†’ Path (X âˆ· Xf) â†’ X
path-head (x :: xs) = x

path-tail : {X : Type} {Xf : X â†’ ğ•‹} ((x :: xs) : Path (X âˆ· Xf)) â†’ Path (Xf x)
path-tail (x :: xs) = xs

plength : {Xt : ğ•‹} â†’ Path Xt â†’ â„•
plength {[]}     âŸ¨âŸ©        = 0
plength {X âˆ· Xf} (x :: xs) = succ (plength {Xf x} xs)

\end{code}

NB. An alternative inductive definition of Path is the following,
where, unfortunately, we get a higher type level, and so we won't use
it:

\begin{code}

data Pathâ‚ : ğ•‹ â†’ Typeâ‚ where
 []  : Pathâ‚ []
 _âˆ·_ : {X : Type} {Xf : X â†’ ğ•‹} (x : X) (xs : Pathâ‚ (Xf x)) â†’ Pathâ‚ (X âˆ· Xf)

\end{code}

Quantifiers as in Section 1 of reference [1]:

\begin{code}

K : Type â†’ Type â†’ Type
K R X = (X â†’ R) â†’ R

\end{code}

In the same way as the type of moves at a given stage of the game
depends on the previously played moves, so do the quantifiers and
selection functions.

ğ“š assigns a quantifier to each node in a given tree:

\begin{code}

ğ“š :  Type â†’ ğ•‹ â†’ Type
ğ“š R []       = ğŸ™
ğ“š R (X âˆ· Xf) = K R X Ã— ((x : X) â†’ ğ“š R (Xf x))

\end{code}

 â‹† Ï•  ranges over the type K R X of quantifiers.
 â‹† Ï•t ranges over the type ğ“š R Xt of quantifier trees.
 â‹† Ï•f ranges over the type (x : X) â†’ ğ“š R (Xf x) of quantifier forests.


Sequencing quantifiers, as constructed in Definition 2 of reference [1],
but using our tree representation of games instead:

\begin{code}

K-sequence : {Xt : ğ•‹} {R : Type} â†’ ğ“š R Xt â†’ K R (Path Xt)
K-sequence {[]}     âŸ¨âŸ©        q = q âŸ¨âŸ©
K-sequence {X âˆ· Xf} {R} (Ï• :: Ï•f) q = Ï• (Î» x â†’ Î³ x (Î» xs â†’ q (x :: xs)))
 where
  Î³ : (x : X) â†’ K R (Path (Xf x))
  Î³ x = K-sequence {Xf x} (Ï•f x)

module remark-about-K-sequence (R : Type) where

 Î·á´· : {X : Type} â†’ X â†’ K R X
 Î·á´· x p = p x

 K-ext : {X Y : Type} â†’ (X â†’ K R Y) â†’ K R X â†’ K R Y
 K-ext f Ï• p = Ï• (Î» x â†’ f x p)

 K-map : {X Y : Type} â†’ (X â†’ Y) â†’ K R X â†’ K R Y
 K-map f = K-ext (Î·á´· âˆ˜ f)

 _âŠ—á´·_ : {X : Type} {Y : X â†’ Type}
      â†’ K R X
      â†’ ((x : X) â†’ K R (Y x))
      â†’ K R (Î£ x ê‰ X , Y x)
 Ï• âŠ—á´· Î³ = K-ext (Î» x â†’ K-map (Î» y â†’ x , y) (Î³ x)) Ï•

 remarká´· : {X : Type} {Xf : X â†’ ğ•‹}
           (Ï• : K R X)
           (Ï•f : (x : X) â†’ ğ“š R (Xf x))
         â†’ K-sequence {X âˆ· Xf} (Ï• :: Ï•f) âˆ¼ Ï• âŠ—á´· (Î» x â†’ K-sequence {Xf x} (Ï•f x))
 remarká´· Ï• f q = refl

\end{code}

The following is Definition 3 of the above reference [1].

A game is defined by a game tree Xt, a type R of outcomes, a
quantifier tree Ï•t and an outcome function q:

\begin{code}

record Game : Typeâ‚ where
 constructor game
 field
  Xt  : ğ•‹
  R   : Type
  q   : Path Xt â†’ R
  Ï•t  : ğ“š R Xt

open Game

\end{code}

We can think of Xt as the rules of the game, and R, Ï•t and q as the
objective of the game.

We define the optimal outcome of a game to be the sequencing of its
quantifiers applied to the outcome function (Theorem 3.1 of [1]).

\begin{code}

optimal-outcome : (G : Game) â†’ R G
optimal-outcome (game R Xt q Ï•t) = K-sequence Ï•t q

\end{code}

A strategy defines how to pick a path of a tree. The type Strategy of
all possible strategies is constructed as follows (Definition 4 of [1]):

\begin{code}

Strategy : ğ•‹ -> Type
Strategy []       = ğŸ™
Strategy (X âˆ· Xf) = X Ã— ((x : X) â†’ Strategy (Xf x))

\end{code}

 â‹† Ïƒ ranges over the type Strategy Xt of strategies for a
   dependent-type tree Xt.

 â‹† Ïƒf ranges over the type (x : X) â†’ Strategy (Xf x) of strategy
   forests for a dependent-type forest Xf.

We get a path in the tree by following any given strategy:

\begin{code}

strategic-path : {Xt : ğ•‹} â†’ Strategy Xt â†’ Path Xt
strategic-path {[]}     âŸ¨âŸ©        = âŸ¨âŸ©
strategic-path {X âˆ· Xf} (x :: Ïƒf) = x :: strategic-path {Xf x} (Ïƒf x)

\end{code}

In the notation of reference [1] above, Definition 5, a strategy Ïƒ
for a game (Xt,R,Ï•t,q) is said to be optimal, or in subgame perfect
equilibrium (abbreviated sgpe), if for every partial play aâ‚€,â€¦,aâ‚–â‚‹â‚
of length k, we have

   q(aâ‚€,â€¦,aâ‚–â‚‹â‚,bâ‚–(aâ‚€,â€¦,aâ‚–â‚‹â‚),â€¦,bâ‚™â‚‹â‚(aâ‚€,â€¦,aâ‚–â‚‹â‚))
 = Ï•â‚–(Î»xâ‚–.q(aâ‚€,â€¦,aâ‚–â‚‹â‚,xâ‚–,bâ‚–â‚Šâ‚(aâ‚€,â€¦,aâ‚–â‚‹â‚,xâ‚–),â€¦,bâ‚™â‚‹â‚(aâ‚€,â€¦,aâ‚–â‚‹â‚,xâ‚–)))

where the sequence b is inductively defined by

  bâ±¼(aâ‚€,â€¦,aâ‚–â‚‹â‚) = Ïƒâ±¼(aâ‚€,â€¦,aâ‚–â‚‹â‚,bâ‚–(aâ‚€,â€¦,aâ‚–â‚‹â‚),â€¦,bâ±¼â‚‹â‚(aâ‚€,â€¦,aâ‚–â‚‹â‚))

for k â‰¤ j < n.

Intuitively, the strategy Ïƒ is optimal if the outcome

  q(aâ‚€,â€¦,aâ‚–â‚‹â‚,bâ‚–(aâ‚€,â€¦,aâ‚–â‚‹â‚),â€¦,bâ‚™â‚‹â‚(aâ‚€,â€¦,aâ‚–â‚‹â‚))

obtained by following Ïƒ is the best possible outcome as described by
the quantifier Ï•â‚– for each round k, considering all possible
deviations xâ‚– from the strategy at that round.

For the purposes of our generalization of [1] to dependent games, it
is convenient to define this notion by induction on the game tree Xt:

\begin{code}

is-sgpe : {Xt : ğ•‹} {R : Type} â†’ ğ“š R Xt â†’ (Path Xt â†’ R) â†’ Strategy Xt â†’ Type
is-sgpe {[]}     âŸ¨âŸ©        q âŸ¨âŸ©         = ğŸ™
is-sgpe {X âˆ· Xf} (Ï• :: Ï•f) q (xâ‚€ :: Ïƒf) =

      (q (xâ‚€ :: strategic-path (Ïƒf xâ‚€)) ï¼ Ï• (Î» x â†’ q (x :: strategic-path (Ïƒf x))))
    Ã—
      ((x : X) â†’ is-sgpe {Xf x} (Ï•f x) (Î» (xs : Path (Xf x)) â†’ q (x :: xs)) (Ïƒf x))

\end{code}

In the above definition:

 â‹† If the game tree is empty, then the strategy is empty, and we say
   that it is true that it is in sgpe, where "true" is represented by
   the unit type ğŸ™ in propositions-as-types.

 â‹† If the game tree has a root X followed by a forest Xf, then the
   strategy must be of the form xâ‚€ :: Ïƒf, where xâ‚€ is the first move
   according to the strategy, and where Ïƒf is a forest of strategies
   that depends on a deviation x.

   So the first part

     q (xâ‚€ :: strategic-path (Ïƒf xâ‚€)) ï¼ Ï• (Î» x â†’ q (x :: strategic-path (Ïƒf x)))

   of the definition is as in the comment above, but with a partial
   play of length k=0, and the second (inductive) part, says that the
   substrategy Ïƒf x, for any deviation x, is in subgame perfect
   equilibrium in the subgame

     (Xf x , R , Ï•f x , Î» (xs : Path (Xf x)) â†’ q (x :: xs)).

As discussed above, we say that a strategy for a game is optimal if it
is in subgame perfect equilibrium.

\begin{code}

is-optimal : (G : Game) (Ïƒ : Strategy (Xt G)) â†’ Type
is-optimal (game Xt R Ï•t q) Ïƒ = is-sgpe {Xt} {R} q Ï•t Ïƒ

\end{code}

The main lemma is that the optimal outcome is the same thing as the
application of the outcome function to the path induced by a strategy
in perfect subgame equilibrium.

The following is Theorem 3.1 of reference [1].

\begin{code}

sgpe-lemma : (Xt : ğ•‹) {R : Type} (Ï•t : ğ“š R Xt) (q : Path Xt â†’ R) (Ïƒ : Strategy Xt)
           â†’ is-sgpe Ï•t q Ïƒ
           â†’ K-sequence Ï•t q ï¼ q (strategic-path Ïƒ)
sgpe-lemma []       âŸ¨âŸ©        q âŸ¨âŸ©        âŸ¨âŸ©       = refl
sgpe-lemma (X âˆ· Xf) (Ï• :: Ï•t) q (a :: Ïƒf) (h :: t) = Î³
 where
  observation-t : type-of t ï¼ ((x : X) â†’ is-sgpe (Ï•t x) (Î» xs â†’ q (x :: xs)) (Ïƒf x))
  observation-t = refl

  IH : (x : X) â†’ K-sequence (Ï•t x) (Î» xs â†’ q (x :: xs)) ï¼ q (x :: strategic-path (Ïƒf x))
  IH x = sgpe-lemma (Xf x) (Ï•t x) (Î» (xs : Path (Xf x)) â†’ q (x :: xs)) (Ïƒf x) (t x)

  Î³ = Ï• (Î» x â†’ K-sequence (Ï•t x) (Î» xs â†’ q (x :: xs))) ï¼âŸ¨ ap Ï• (dfunext fe IH) âŸ©
      Ï• (Î» x â†’ q (x :: strategic-path (Ïƒf x)))         ï¼âŸ¨ h â»Â¹ âŸ©
      q (a :: strategic-path (Ïƒf a))                   âˆ

\end{code}

This can be reformulated as follows in terms of the type of games:

\begin{code}

equilibrium-theorem : (G : Game) (Ïƒ : Strategy (Xt G))
                    â†’ is-optimal G Ïƒ
                    â†’ optimal-outcome G ï¼ q G (strategic-path Ïƒ)
equilibrium-theorem (game Xt R Ï•t q) = sgpe-lemma Xt q Ï•t

\end{code}

We now show how to use selection functions to compute a sgpe strategy.

Selection functions, as in Section 2 of reference [1]:

\begin{code}

J : Type â†’ Type â†’ Type
J R X = (X â†’ R) â†’ X

\end{code}

ğ“™ assigns selection functions to the nodes.

\begin{code}

ğ“™ :  Type â†’ ğ•‹ â†’ Type
ğ“™ R []       = ğŸ™
ğ“™ R (X âˆ· Xf) = J R X Ã— ((x : X) â†’ ğ“™ R (Xf x))

\end{code}

 â‹† Îµ ranges over the type J R X of selection functions.
 â‹† Îµt ranges over the type ğ“™ R Xt of selection-function trees.
 â‹† Îµf ranges over the type (x : X) â†’ ğ“™ R (Xf x) of selection-function forests.

Sequencing selection functions, as constructed in Definition 12 of
reference [1], but using our tree representation of games instead:

\begin{code}

J-sequenceâ‚€ : {Xt : ğ•‹} {R : Type} â†’ ğ“™ R Xt â†’ J R (Path Xt)
J-sequenceâ‚€ {[]}     âŸ¨âŸ©        q = âŸ¨âŸ©
J-sequenceâ‚€ {X âˆ· Xf} (Îµ :: Îµf) q = h :: t h
 where
  t : (x : X) â†’ Path (Xf x)
  t x = J-sequenceâ‚€ {Xf x} (Îµf x) (Î» xs â†’ q (x :: xs))

  h : X
  h = Îµ (Î» x â†’ q (x :: t x))

module remark-about-J-sequence (R : Type) where

 Î·á´¶ : {X : Type} â†’ X â†’ J R X
 Î·á´¶ x p = x

 J-ext : {X Y : Type} â†’ (X â†’ J R Y) â†’ J R X â†’ J R Y
 J-ext f Îµ p = f (Îµ (Î» x â†’ p (f x p))) p

 J-map : {X Y : Type} â†’ (X â†’ Y) â†’ J R X â†’ J R Y
 J-map f = J-ext (Î·á´¶ âˆ˜ f)

 _âŠ—á´¶_ : {X : Type} {Y : X â†’ Type}
      â†’ J R X
      â†’ ((x : X) â†’ J R (Y x))
      â†’ J R (Î£ x ê‰ X , Y x)
 Ï• âŠ—á´¶ Î´ = J-ext (Î» x â†’ J-map (Î» y â†’ x , y) (Î´ x)) Ï•

 remarká´¶ : {X : Type} {Xf : X â†’ ğ•‹}
           (Ï• : J R X)
           (Ï•f : (x : X) â†’ ğ“™ R (Xf x))
         â†’ J-sequenceâ‚€ {X âˆ· Xf} (Ï• :: Ï•f) âˆ¼ Ï• âŠ—á´¶ (Î» x â†’ J-sequenceâ‚€ {Xf x} (Ï•f x))
 remarká´¶ Ï• f q = refl

\end{code}

Try to make faster, exploiting Agda's evaluation strategy, but this
doesn't seem to make any difference:

\begin{code}

J-sequenceâ‚ : {Xt : ğ•‹} {R : Type} â†’ ğ“™ R Xt â†’ J R (Path Xt)
J-sequenceâ‚ {[]}     âŸ¨âŸ©        q = âŸ¨âŸ©
J-sequenceâ‚ {X âˆ· Xf} (Îµ :: Îµf) q = Î³
 where
  t : (x : X) â†’ Path (Xf x)
  t x = J-sequenceâ‚ {Xf x} (Îµf x) (Î» xs â†’ q (x :: xs))

  Î½ : X â†’ Path (X âˆ· Xf)
  Î½ x = x :: t x

  xâ‚€ : X
  xâ‚€ = Îµ (Î» x â†’ q (Î½ x))

  Î³ : Path (X âˆ· Xf)
  Î³ = Î½ xâ‚€

\end{code}

Or this:

\begin{code}

J-sequenceâ‚‚ : {Xt : ğ•‹} {R : Type} â†’ ğ“™ R Xt â†’ J R (Path Xt)
J-sequenceâ‚‚ {[]}     _         q = âŸ¨âŸ©
J-sequenceâ‚‚ {X âˆ· Xf} (Îµ :: Îµf) q = Î½ (Îµ (Î» x â†’ q (Î½ x)))
 where
  Î½ : X â†’ Path (X âˆ· Xf)
  Î½ x = x :: J-sequenceâ‚‚ {Xf x} (Îµf x) (Î» xs â†’ q (x :: xs))

J-sequence = J-sequenceâ‚‚

\end{code}

We now convert a selection function into a quantifier as in
Definition 10 of [1]:

\begin{code}

overline : {X R : Type} â†’ J R X â†’ K R X
overline Îµ = Î» p â†’ p (Îµ p)

\end{code}

The following is the application of overline to each selection
function of a tree:

\begin{code}

Overline : {Xt : ğ•‹} {R : Type} â†’ ğ“™ R Xt â†’ ğ“š R Xt
Overline {[]}     âŸ¨âŸ©        = âŸ¨âŸ©
Overline {X âˆ· Xf} (Îµ :: Îµs) = overline Îµ :: (Î» x â†’ Overline {Xf x} (Îµs x))

\end{code}

The following, which defines a strategy from given selection
functions, is defined in Theorem 5.4 of [1], with the difference that
here, for the moment, we consider only single-valued quantifiers.

\begin{code}

selection-strategy : {Xt : ğ•‹} {R : Type} â†’ ğ“™ R Xt â†’ (Path Xt â†’ R) â†’ Strategy Xt
selection-strategy {[]}     âŸ¨âŸ©           q = âŸ¨âŸ©
selection-strategy {X âˆ· Xf} Îµt@(Îµ :: Îµf) q = xâ‚€ :: Ïƒf
 where
  xâ‚€ : X
  xâ‚€ = path-head (J-sequence Îµt q)

  Ïƒf : (x : X) â†’ Strategy (Xf x)
  Ïƒf x = selection-strategy {Xf x} (Îµf x) (Î» xs â†’ q (x :: xs))

\end{code}

The following definition is in Section 1 on [1].

\begin{code}

_is-a-selection-of_ : {X R : Type} â†’ J R X â†’ K R X â†’ Type
Îµ is-a-selection-of Ï• = overline Îµ âˆ¼ Ï•

\end{code}

We generalize it to selection-function and quantifier trees in the
obvious way, by induction:

\begin{code}

_are-selections-of_ : {Xt : ğ•‹} {R : Type} â†’ ğ“™ R Xt â†’ ğ“š R Xt â†’ Type
_are-selections-of_ {[]}     âŸ¨âŸ©        âŸ¨âŸ©        = ğŸ™
_are-selections-of_ {X âˆ· Xf} (Îµ :: Îµf) (Ï• :: Ï•f) = (Îµ is-a-selection-of Ï•)
                                                 Ã— ((x : X) â†’ (Îµf x) are-selections-of (Ï•f x))

observation : {Xt : ğ•‹} {R : Type} (Îµt : ğ“™ R Xt) (Ï•t : ğ“š R Xt)
            â†’ Îµt are-selections-of Ï•t
            â†’ Overline Îµt ï¼ Ï•t
observation {[]}     âŸ¨âŸ©        âŸ¨âŸ©        âŸ¨âŸ©        = refl
observation {X âˆ· Xf} (Îµ :: Îµf) (Ï• :: Ï•f) (a :: af) = Î³
 where
  IH : (x : X) â†’ Overline (Îµf x) ï¼ Ï•f x
  IH x = observation {Xf x} (Îµf x) (Ï•f x) (af x)

  I : overline Îµ ï¼ Ï•
  I = dfunext fe a

  II : (Î» x â†’ Overline (Îµf x)) ï¼ Ï•f
  II = dfunext fe IH

  Î³ : overline Îµ :: (Î» x â†’ Overline (Îµf x)) ï¼ Ï• :: Ï•f
  Î³ = apâ‚‚ _::_ I II

\end{code}

Notice that the converse is also true, that is, if Overline Îµt ï¼ Ï•t
then Îµt are selections of Ï•t, but we don't need this fact here.

\begin{code}

crucial-lemma : {Xt : ğ•‹} {R : Type} (Îµt : ğ“™ R Xt) (q : Path Xt â†’ R)
              â†’ J-sequence Îµt q
              ï¼ strategic-path (selection-strategy Îµt q)
crucial-lemma {[]}     âŸ¨âŸ©           q = refl
crucial-lemma {X âˆ· Xf} Îµt@(Îµ :: Îµf) q = Î³
 where
  t : (x : X) â†’ Path (Xf x)
  t x = J-sequence {Xf x} (Îµf x) (Î» xs â†’ q (x :: xs))

  xâ‚€ : X
  xâ‚€ = path-head (J-sequence Îµt q)

  remark-used-implicitly : xâ‚€ ï¼ Îµ (Î» x â†’ q (x :: t x))
  remark-used-implicitly = refl

  Ïƒf : (x : X) â†’ Strategy (Xf x)
  Ïƒf x = selection-strategy {Xf x} (Îµf x) (Î» xs â†’ q (x :: xs))

  IH : t xâ‚€ ï¼ strategic-path (Ïƒf xâ‚€)
  IH = crucial-lemma (Îµf xâ‚€) (Î» xs â†’ q (xâ‚€ :: xs))

  Î³ : xâ‚€ :: t xâ‚€ ï¼ xâ‚€ :: strategic-path (Ïƒf xâ‚€)
  Î³ = ap (xâ‚€ ::_) IH

selection-strategy-lemma : {Xt : ğ•‹} {R : Type} (Îµt : ğ“™ R Xt) (q : Path Xt â†’ R)
                         â†’ is-sgpe (Overline Îµt) q (selection-strategy Îµt q)
selection-strategy-lemma {[]}     {R} âŸ¨âŸ©        q = âŸ¨âŸ©
selection-strategy-lemma {X âˆ· Xf} {R} (Îµ :: Îµf) q = h :: t
 where
  f g : X â†’ R
  f x = q (x :: J-sequence (Îµf x) (Î» xs â†’ q (x :: xs)))
  g x = q (x :: strategic-path (selection-strategy (Îµf x) (Î» xs â†’ q (x :: xs))))

  I : (x : X) â†’ J-sequence (Îµf x) (Î» xs â†’ q (x :: xs))
              ï¼ strategic-path (selection-strategy (Îµf x) (Î» xs â†’ q (x :: xs)))
  I x = crucial-lemma (Îµf x) (Î» xs â†’ q (x :: xs))

  II : f ï¼ g
  II = dfunext fe (Î» x â†’ ap (Î» - â†’ q (x :: -)) (I x))

  h : g (Îµ f) ï¼ g (Îµ g)
  h = ap (g âˆ˜ Îµ) II

  t : (x : X) â†’ is-sgpe
                  (Overline (Îµf x))
                  (Î» xs â†’ q (x :: xs))
                  (selection-strategy (Îµf x) (Î» xs â†’ q (x :: xs)))
  t x = selection-strategy-lemma (Îµf x) (Î» xs â†’ q (x :: xs))

\end{code}

The following, which shows how to use selection functions to compute
optimal strategies, corresponds to Theorem 6.2 of [1].

\begin{code}

selection-strategy-theorem : {Xt : ğ•‹} {R : Type} (Îµt : ğ“™ R Xt) (Ï•t : ğ“š R Xt) (q : Path Xt â†’ R)
                           â†’ Îµt are-selections-of Ï•t
                           â†’ is-sgpe Ï•t q (selection-strategy Îµt q)
selection-strategy-theorem Îµt Ï•t q a = III
 where
  I : Overline Îµt ï¼ Ï•t
  I = observation Îµt Ï•t a

  II : is-sgpe (Overline Îµt) q (selection-strategy Îµt q)
  II = selection-strategy-lemma Îµt q

  III : is-sgpe Ï•t q (selection-strategy Îµt q)
  III = transport (Î» - â†’ is-sgpe - q (selection-strategy Îµt q)) I II


Selection-Strategy-Theorem : (G : Game) (Îµt : ğ“™ (R G) (Xt G))
                           â†’ Îµt are-selections-of (Ï•t G)
                           â†’ is-optimal G (selection-strategy Îµt (q G))
Selection-Strategy-Theorem (game Xt R Ï•t q) Îµt = selection-strategy-theorem Îµt q Ï•t

\end{code}

Incomplete example:

\begin{code}

module permutations-example where

 open import MLTT.NonSpartanMLTTTypes

 no-repetitions : (n : â„•) (X : Type) â†’ ğ•‹
 no-repetitions 0        X = []
 no-repetitions (succ n) X = X âˆ· Î» (x : X) â†’ no-repetitions n (Î£ y ê‰ X , y â‰  x)

 Permutations : â„• â†’ Type
 Permutations n = Path (no-repetitions n (Fin n))

 example-permutation2 : Permutations 2
 example-permutation2 = ğŸ :: ((ğŸ , (Î» ())) :: âŸ¨âŸ©)

 example-permutation3 : Permutations 3
 example-permutation3 = ğŸ :: ((ğŸ :: (Î» ())) :: (((ğŸ , (Î» ())) , (Î» ())) :: âŸ¨âŸ©))

\end{code}

We use the type GameJ to present games equipped with selection
functions, as in some examples, such as tic-tac-toe this is easier
than to give a game directly.

\begin{code}

data GameJ (R : Type) : Typeâ‚ where
  leaf   : R â†’ GameJ R
  branch : (X : Type) (Xf : X â†’ GameJ R) (Îµ : J R X) â†’ GameJ R


dtt : {R : Type} â†’ GameJ R â†’ ğ•‹
dtt (leaf x)        = []
dtt (branch X Xf Îµ) = X âˆ· Î» x â†’ dtt (Xf x)

predicate : {R : Type} (Î“ : GameJ R) â†’ Path (dtt Î“) â†’ R
predicate (leaf r)        âŸ¨âŸ©        = r
predicate (branch X Xf Îµ) (x :: xs) = predicate (Xf x) xs

selections : {R : Type} (Î“ : GameJ R) â†’ ğ“™ R (dtt Î“)
selections (leaf r)        = âŸ¨âŸ©
selections (branch X Xf Îµ) = Îµ :: (Î» x â†’ selections (Xf x))

quantifiers : {R : Type} (Î“ : GameJ R) â†’ ğ“š R (dtt Î“)
quantifiers (leaf r)        = âŸ¨âŸ©
quantifiers (branch X Xf Îµ) = overline Îµ :: (Î» x â†’ quantifiers (Xf x))

Game-from-GameJ : {R : Type} â†’ GameJ R â†’ Game
Game-from-GameJ {R} Î“ = game (dtt Î“) R (predicate Î“) (quantifiers Î“)

strategyJ : {R : Type} (Î“ : GameJ R) â†’ Strategy (dtt Î“)
strategyJ Î“ = selection-strategy (selections Î“) (predicate Î“)

Selection-Strategy-TheoremJ : {R : Type} (Î“ : GameJ R)
                            â†’ is-optimal (Game-from-GameJ Î“) (strategyJ Î“)
Selection-Strategy-TheoremJ {R} Î“ = Î³
 where
  Î´ : (Î“ : GameJ R) â†’ (selections Î“) are-selections-of (quantifiers Î“)
  Î´ (leaf r)        = âŸ¨âŸ©
  Î´ (branch X Xf Îµ) = (Î» p â†’ refl) , (Î» x â†’ Î´ (Xf x))

  Î³ : is-optimal (Game-from-GameJ Î“) (strategyJ Î“)
  Î³ = Selection-Strategy-Theorem (Game-from-GameJ Î“) (selections Î“) (Î´ Î“)

\end{code}

The following is used in conjunction with GameJ to build certain games
in a convenient way.

\begin{code}

build-GameJ : {R          : Type}
              (draw       : R)
              (Board      : Type)
              (transition : Board â†’ R + (Î£ M ê‰ Type , (M â†’ Board) Ã— J R M))
              (n          : â„•)
              (b          : Board)
            â†’ GameJ R
build-GameJ {R} draw Board transition n b = h n b
 where
  h : â„• â†’ Board â†’ GameJ R
  h 0        b = leaf draw
  h (succ n) b = g (transition b) refl
   where
    g : (f : R + (Î£ M ê‰ Type , (M â†’ Board) Ã— J R M)) â†’ transition b ï¼ f â†’ GameJ R
    g (inl r)              p = leaf r
    g (inr (M , play , Îµ)) p = branch M Xf Îµ
     where
      Xf : M â†’ GameJ R
      Xf m = h n (play m)

build-Game : {R          : Type}
             (draw       : R)
             (Board      : Type)
             (transition : Board â†’ R + (Î£ M ê‰ Type , (M â†’ Board) Ã— J R M))
             (n          : â„•)
             (b          : Board)
           â†’ Game
build-Game draw Board transition n b = Game-from-GameJ (build-GameJ draw Board transition n b)

\end{code}

Example: Tic-tac-toe. We have two versions.

\begin{code}

tic-tac-toeâ‚ : Game
tic-tac-toeâ‚ = build-Game draw Board transition 9 boardâ‚€
 where
  open import TypeTopology.CompactTypes
  open import UF.Subsingletons
  open import TypeTopology.DiscreteAndSeparated
  open import UF.Miscelanea

  open import MLTT.NonSpartanMLTTTypes hiding (Fin ; ğŸ ; ğŸ ; ğŸ ; ğŸ‘ ; ğŸ’ ; ğŸ“ ; ğŸ” ; ğŸ• ; ğŸ– ; ğŸ—)
  open import MLTT.Fin
  open import MLTT.Fin-Properties

  data Player : Type where
   X O : Player

  opponent : Player â†’ Player
  opponent X = O
  opponent O = X

  ğŸ› = Fin 3

  pattern X-wins = ğŸ
  pattern draw   = ğŸ
  pattern O-wins = ğŸ

  Grid   = ğŸ› Ã— ğŸ›
  Matrix = Grid â†’ Maybe Player
  Board  = Player Ã— Matrix

\end{code}

Convention: in a board (p , A), p is the opponent of the the current player.

\begin{code}

  Grid-is-discrete : is-discrete Grid
  Grid-is-discrete = Ã—-is-discrete Fin-is-discrete Fin-is-discrete

  Grid-compact : Compact Grid {ğ“¤â‚€}
  Grid-compact = Ã—-Compact Fin-Compact Fin-Compact

  boardâ‚€ : Board
  boardâ‚€ = X , (Î» _ â†’ Nothing)

  Move : Board â†’ Type
  Move (_ , A) = Î£ g ê‰ Grid , A g ï¼ Nothing

  Move-decidable : (b : Board) â†’ decidable (Move b)
  Move-decidable (_ , A) = Grid-compact
                            (Î» g â†’ A g ï¼ Nothing)
                            (Î» g â†’ Nothing-is-isolated' (A g))

  Move-compact : (b : Board) â†’ Compact (Move b)
  Move-compact (x , A) = complemented-subset-of-compact-type
                          Grid-compact
                          (Î» g â†’ Nothing-is-isolated' (A g))
                          (Î» g â†’ Nothing-is-h-isolated' (A g))

  selection : (b : Board) â†’ Move b â†’ J ğŸ› (Move b)
  selection b@(X , A) m p = prâ‚ (compact-argmax p (Move-compact b) m)
  selection b@(O , A) m p = prâ‚ (compact-argmin p (Move-compact b) m)

  _is_ : Maybe Player â†’ Player â†’ Bool
  Nothing is _ = false
  Just X  is X = true
  Just O  is X = false
  Just X  is O = false
  Just O  is O = true

  infix 30 _is_

  wins : Player â†’ Matrix â†’ Bool
  wins p A = line || col || diag
   where
    lâ‚€ = A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p
    lâ‚ = A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p
    lâ‚‚ = A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p

    câ‚€ = A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p
    câ‚ = A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p
    câ‚‚ = A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p

    dâ‚€ = A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p
    dâ‚ = A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p && A (ğŸ , ğŸ) is p

    line = lâ‚€ || lâ‚ || lâ‚‚
    col  = câ‚€ || câ‚ || câ‚‚
    diag = dâ‚€ || dâ‚

  update : (p : Player) (A : Matrix)
         â†’ Move (p , A) â†’ Matrix
  update p A (m , _) m' = f (Grid-is-discrete m m')
   where
    f : decidable (m ï¼ m') â†’ Maybe Player
    f (inl _) = Just p
    f (inr _) = A m'

  play : (b : Board) (m : Move b) â†’ Board
  play (p , A) m = opponent p , update p A m

  transition : Board â†’ ğŸ› + (Î£ M ê‰ Type , (M â†’ Board) Ã— J ğŸ› M)
  transition (p , A) = f p A (wins p A) refl
   where
    f : (p : Player) (A : Matrix) (b : Bool) â†’ wins p A ï¼ b
      â†’ ğŸ› + (Î£ M ê‰ Type , (M â†’ Board) Ã— J ğŸ› M)
    f X A true e  = inl X-wins
    f O A true e  = inl O-wins
    f p A false e = Cases (Move-decidable (p , A))
                     (Î» (g , e) â†’ inr (Move (p , A) ,
                                       (Î» m â†’ opponent p , update p A m) ,
                                       selection (p , A) (g , e)))
                     (Î» Î½ â†’ inl draw)

tâ‚ : R tic-tac-toeâ‚
tâ‚ = optimal-outcome tic-tac-toeâ‚

\end{code}

The above computation takes too long, due to the use of brute-force search.

The following is another, more efficient, version of tic-tac-toe, with
a more refined exhaustive search that allows us to compute answers.

\begin{code}

data ğŸ› : Type where
 O-wins draw X-wins : ğŸ›

tic-tac-toeâ‚‚J : GameJ ğŸ›
tic-tac-toeâ‚‚J = build-GameJ draw Board transition 9 boardâ‚€
 where
  flip : ğŸ› â†’ ğŸ›
  flip O-wins = X-wins
  flip draw   = draw
  flip X-wins = O-wins

  data Player : Type where
   O X : Player

  open import MLTT.NonSpartanMLTTTypes
  open list-util

  Cell = Fin 9

  record Board : Type where
   pattern
   constructor board
   field
    next-player     : Player
    available-moves : List Cell
    X-moves         : List Cell
    O-moves         : List Cell

  open Board

  opponent-wins : Player â†’ ğŸ›
  opponent-wins X = O-wins
  opponent-wins O = X-wins

  winning : List Cell â†’ Bool
  winning = some-contained ((ğŸ âˆ· ğŸ âˆ· ğŸ âˆ· [])
                          âˆ· (ğŸ‘ âˆ· ğŸ’ âˆ· ğŸ“ âˆ· [])
                          âˆ· (ğŸ” âˆ· ğŸ• âˆ· ğŸ– âˆ· [])
                          âˆ· (ğŸ âˆ· ğŸ‘ âˆ· ğŸ” âˆ· [])
                          âˆ· (ğŸ âˆ· ğŸ’ âˆ· ğŸ• âˆ· [])
                          âˆ· (ğŸ âˆ· ğŸ“ âˆ· ğŸ– âˆ· [])
                          âˆ· (ğŸ âˆ· ğŸ’ âˆ· ğŸ– âˆ· [])
                          âˆ· (ğŸ âˆ· ğŸ’ âˆ· ğŸ” âˆ· [])
                          âˆ· [])

  wins : Board â†’ Bool
  wins (board O _ _  os) = winning os
  wins (board X _ xs  _) = winning xs

  boardâ‚€ : Board
  boardâ‚€ = board X (list-Fin 9) [] []

  Move : List Cell â†’ Type
  Move xs = Î£ c ê‰ Cell , ((c is-in xs) ï¼ true)

\end{code}

The following definition of argmax is somewhat convoluted because it
is optimized for time, by minimizing the number of evaluations of the
predicate q:

\begin{code}

  argmax : (m : Cell) (ms : List Cell) â†’ ğŸ› â†’ (Move (m âˆ· ms) â†’ ğŸ›) â†’ Move (m âˆ· ms)
  argmax m ms       X-wins  q = m , need m == m || (m is-in ms) ï¼ true
                                    which-is-given-by ||-left-intro _ (==-refl m)

  argmax m []       r       q = m , need m == m || (m is-in []) ï¼ true
                                    which-is-given-by ||-left-intro _ (==-refl m)

  argmax m (x âˆ· xs) O-wins  q = Î¹ Î³
   where
    Î¹ : Move (x âˆ· xs) â†’ Move (m âˆ· x âˆ· xs)
    Î¹ (c , e) = c , need c == m || (c is-in (x âˆ· xs)) ï¼ true
                    which-is-given-by ||-right-intro {c == m} _ e

    q' : Move (x âˆ· xs) â†’ ğŸ›
    q' m = q (Î¹ m)

    a : (x == m) || ((x == x) || (x is-in xs)) ï¼ true
    a = ||-right-intro {x == m} _ (||-left-intro _ (==-refl x))

    Î³ : Move (x âˆ· xs)
    Î³ = argmax x xs (q (x , a)) q'

  argmax m us@(x âˆ· ms) draw q = g us c
   where
    c : ((x == x) || (x is-in ms)) && (ms contained-in (x âˆ· ms)) ï¼ true
    c = &&-intro (||-left-intro _ (==-refl x)) (contained-lemmaâ‚ x ms)

    g : (vs : List Cell) â†’ vs contained-in us ï¼ true â†’ Move (m âˆ· us)
    g []       c = m , need m == m || (m is-in (x âˆ· ms)) ï¼ true
                       which-is-given-by ||-left-intro _ (==-refl m)

    g (y âˆ· vs) c = k (q (y , a))
     where
      a : (y == m) || ((y == x) || (y is-in ms)) ï¼ true
      a = ||-right-intro {y == m} _ (prâ‚ (&&-gives-Ã— c))

      b : (vs contained-in (x âˆ· ms)) ï¼ true
      b = prâ‚‚ (&&-gives-Ã— c)

      k : ğŸ› â†’ Move (m âˆ· us)
      k X-wins = y , a
      k r      = g vs b

  argmin : (m : Cell) (ms : List Cell) â†’ ğŸ› â†’ (Move (m âˆ· ms) â†’ ğŸ›) â†’ Move (m âˆ· ms)
  argmin m ms r q = argmax m ms (flip r) (Î» xs â†’ flip (q xs))

  arg : Player â†’ (ms : List Cell) â†’ empty ms ï¼ false â†’  J ğŸ› (Move ms)
  arg _ []       e q = ğŸ˜-elim (true-is-not-false e)
  arg X (m âˆ· ms) e q = argmax m ms (q (m , ||-left-intro (m is-in ms) (==-refl m))) q
  arg O (m âˆ· ms) e q = argmin m ms (q (m , ||-left-intro (m is-in ms) (==-refl m))) q

  play : (b : Board) â†’ Move (available-moves b) â†’ Board
  play (board X as xs os) (c , e) = board O (remove-first c as) (insert c xs) os
  play (board O as xs os) (c , e) = board X (remove-first c as) xs            (insert c os)

  transition : Board â†’ ğŸ› + (Î£ M ê‰ Type , (M â†’ Board) Ã— J ğŸ› M)
  transition b@(board next as xs os) =
   if wins b
   then inl (opponent-wins next)
   else Bool-equality-cases (empty as)
         (Î» (_ : empty as ï¼ true)  â†’ inl draw)
         (Î» (e : empty as ï¼ false) â†’ inr (Move as , play b , arg next as e))

tic-tac-toeâ‚‚ : Game
tic-tac-toeâ‚‚ = Game-from-GameJ tic-tac-toeâ‚‚J

tâ‚‚ : R tic-tac-toeâ‚‚
tâ‚‚ = optimal-outcome tic-tac-toeâ‚‚

sâ‚‚ : Path (Xt tic-tac-toeâ‚‚)
sâ‚‚ = strategic-path (selection-strategy (selections tic-tac-toeâ‚‚J) (q tic-tac-toeâ‚‚))

uâ‚‚ : Path (Xt tic-tac-toeâ‚‚)
uâ‚‚ = J-sequence (selections tic-tac-toeâ‚‚J) (q tic-tac-toeâ‚‚)

lâ‚‚ : â„•
lâ‚‚ = plength sâ‚‚

{- Slow

tâ‚‚-test : tâ‚‚ ï¼ draw
tâ‚‚-test = refl

-}

{- Slow:

lâ‚‚-test : lâ‚‚ ï¼ 9
lâ‚‚-test = refl

-}

{- slow

open import NonSpartanMLTTTypes

uâ‚‚-test : sâ‚‚ ï¼ (ğŸ :: refl)
           :: ((ğŸ’ :: refl)
           :: ((ğŸ :: refl)
           :: ((ğŸ :: refl)
           :: ((ğŸ” :: refl)
           :: ((ğŸ‘ :: refl)
           :: ((ğŸ“ :: refl)
           :: ((ğŸ• :: refl)
           :: ((ğŸ– :: refl)
           :: âŸ¨âŸ©))))))))
uâ‚‚-test = refl
-}

\end{code}

More tests.

\begin{code}

module test where

 open import MLTT.NonSpartanMLTTTypes

 Îµâ‚‚ : J Bool Bool
 Îµâ‚‚ p = p true

 h : â„• â†’ ğ•‹
 h 0        = []
 h (succ n) = Bool âˆ· Î» _ â†’ h n

 Îµs : (n : â„•) â†’ ğ“™ Bool (h n)
 Îµs 0        = âŸ¨âŸ©
 Îµs (succ n) = Îµâ‚‚ :: Î» _ â†’ Îµs n

 Îµ : (n : â„•) â†’ J Bool (Path (h n))
 Îµ n = J-sequence (Îµs n)

 qq : (n : â„•) â†’ Path (h n) â†’ Bool
 qq 0        âŸ¨âŸ©        = true
 qq (succ n) (x :: xs) = not x && qq n xs

 test : (n : â„•) â†’ Path (h n)
 test n = Îµ n (qq n)

\end{code}

TODO. Generalize the above to multi-valued quantifiers, as in [1], using monads.

\begin{code}

data GameK (R : Type) : Typeâ‚ where
  leaf   : R â†’ GameK R
  branch : (X : Type) (Xf : X â†’ GameK R) (Ï• : K R X) â†’ GameK R

\end{code}

TODO. GameK â‰ƒ Game and we have a map GameJ â†’ GameK.

TODO. Define game isomorphism (and possibly homomorphism more generally).

\begin{code}

data ğ•‹' (X : Type) : Typeâ‚ where
  []  : ğ•‹' X
  _âˆ·_ : (A : X â†’ Type) (Xf : (x : X) â†’ A x â†’ ğ•‹' X) â†’ ğ•‹' X

record Gameâ» : Typeâ‚ where
 constructor gameâ»
 field
  Xt  : ğ•‹
  R   : Type
  q   : Path Xt â†’ R

\end{code}

TODO. Gameâ» â‰ƒ (Î£ R : Type, Dğ•‹ R) for a suitable definition of
Dğ•‹. Idea: In Gameâ», we know how to play the game, but we don't know
what the objective of the game is.
