Martin Escardo, Paulo Oliva, 2-27 July 2021

We study finite, history dependent games of perfect information using
selection functions and dependent-type trees.

This is based on our previous work.

  [1] M.H. Escard√≥ and Paulo Oliva. Sequential Games and Optimal
      Strategies. Proceedings of the Royal Society A, 467:1519-1545,
      2011. https://doi.org/10.1098/rspa.2010.0471

We generalize [1] by considering the case that the type X‚Çñ of moves x‚Çñ
at round k depends on the moves played at the previous rounds. So in a
sequence of moves x‚ÇÄ,x‚ÇÅ,x‚ÇÇ,‚Ä¶, we have that

  * the type X‚ÇÄ of initial moves doesn't depend on anything,
  * the type X‚ÇÅ depends on the first move x‚ÇÄ : X‚ÇÄ,
  * the type X‚ÇÇ depends on the first move x‚ÇÄ : X‚ÇÄ and the
    second move x‚ÇÅ : X‚ÇÅ.
  ‚Ä¶

In order to achieve this generalization, we work with game trees
whose nodes are labelled by types that collect the allowed moves at a
given round. Then a play x‚ÇÄ,x‚ÇÅ,x‚ÇÇ,‚Ä¶, is a path in such a tree.

This formulation of the notion of game naturally accounts for finite
games of *unbounded* length, which in [1] was achieved by continuous,
infinite games instead.

We assume a given type R of outcomes for games as a module parameter.

\begin{code}

{-# OPTIONS --without-K --safe --no-sized-types --no-guardedness --auto-inline #-} -- --exact-split

open import MLTT.Spartan hiding (J)

module Games.FiniteHistoryDependent (R : Type) where

open import Games.Monad
open import Games.Base
open import Games.J
open import Games.K
open import Games.JK
open import UF.Base
open import UF.FunExt

open K-definitions R
open J-definitions R

\end{code}

The following module defines the main data structure we use in order
to represent the above kind of game:

\begin{code}

open import Games.TypeTrees

\end{code}

We use quantifiers as in Section 1 of reference [1], defined in
another module.

In the same way as the type of moves at a given stage of the game
depends on the previously played moves, so do the quantifiers and
selection functions.

ùìö assigns a quantifier to each node in a given tree:

\begin{code}

ùìö : ùïã ‚Üí Type
ùìö []       = ùüô
ùìö (X ‚à∑ Xf) = K X √ó ((x : X) ‚Üí ùìö (Xf x))

\end{code}

 * œï  ranges over the type K X of quantifiers.
 * œït ranges over the type ùìö Xt of quantifier trees.
 * œïf ranges over the type (x : X) ‚Üí ùìö (Xf x) of quantifier forests.


Sequencing quantifiers, as constructed in Definition 2 of reference [1],
but using our tree representation of games instead:

\begin{code}

K-sequence : {Xt : ùïã} ‚Üí ùìö Xt ‚Üí K (Path Xt)
K-sequence {[]}     ‚ü®‚ü©        = Œª q ‚Üí q ‚ü®‚ü©
K-sequence {X ‚à∑ Xf} (œï :: œïf) = œï ‚äó·¥∑ (Œª x ‚Üí K-sequence {Xf x} (œïf x))

\end{code}

The following is Definition 3 of the above reference [1].

A game is defined by a game tree Xt, a type R of outcomes, a
quantifier tree œït and an outcome function q:

\begin{code}

record Game : Type‚ÇÅ where
 constructor game
 field
  Xt : ùïã
  q  : Path Xt ‚Üí R
  œït : ùìö Xt

open Game public

\end{code}

We can think of Xt as the rules of the game, and R, œït and q as the
objective of the game.

We define the optimal outcome of a game to be the sequencing of its
quantifiers applied to the outcome function (Theorem 3.1 of [1]).

\begin{code}

optimal-outcome : Game ‚Üí R
optimal-outcome (game Xt q œït) = K-sequence œït q

\end{code}

A strategy defines how to pick a path of a tree. The type Strategy of
all possible strategies is constructed as follows (Definition 4 of [1]):

\begin{code}

Strategy : ùïã -> Type
Strategy []       = ùüô
Strategy (X ‚à∑ Xf) = X √ó ((x : X) ‚Üí Strategy (Xf x))

\end{code}

 * œÉ ranges over the type Strategy Xt of strategies for a
   dependent-type tree Xt.

 * œÉf ranges over the type (x : X) ‚Üí Strategy (Xf x) of strategy
   forests for a dependent-type forest Xf.

We get a path in the tree by following any given strategy:

\begin{code}

strategic-path : {Xt : ùïã} ‚Üí Strategy Xt ‚Üí Path Xt
strategic-path {[]}     ‚ü®‚ü©        = ‚ü®‚ü©
strategic-path {X ‚à∑ Xf} (x :: œÉf) = x :: strategic-path {Xf x} (œÉf x)

\end{code}

In the notation of reference [1] above, Definition 5, a strategy œÉ
for a game (Xt,R,œït,q) is said to be optimal, or in subgame perfect
equilibrium (abbreviated sgpe), if for every partial play a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ
of length k, we have

   q(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,b‚Çñ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ),‚Ä¶,b‚Çô‚Çã‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ))
 = œï‚Çñ(Œªx‚Çñ.q(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,x‚Çñ,b‚Çñ‚Çä‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,x‚Çñ),‚Ä¶,b‚Çô‚Çã‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,x‚Çñ)))

where the sequence b is inductively defined by

  b‚±º(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ) = œÉ‚±º(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,b‚Çñ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ),‚Ä¶,b‚±º‚Çã‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ))

for k ‚â§ j < n.

Intuitively, the strategy œÉ is optimal if the outcome

  q(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,b‚Çñ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ),‚Ä¶,b‚Çô‚Çã‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ))

obtained by following œÉ is the best possible outcome as described by
the quantifier œï‚Çñ for each round k, considering all possible
deviations x‚Çñ from the strategy at that round.

For the purposes of our generalization of [1] to dependent games, it
is convenient to define this notion by induction on the game tree Xt:

\begin{code}

is-sgpe : {Xt : ùïã} ‚Üí ùìö Xt ‚Üí (Path Xt ‚Üí R) ‚Üí Strategy Xt ‚Üí Type
is-sgpe {[]}     ‚ü®‚ü©        q ‚ü®‚ü©         = ùüô
is-sgpe {X ‚à∑ Xf} (œï :: œïf) q (x‚ÇÄ :: œÉf) =

      (sub q x‚ÇÄ (strategic-path (œÉf x‚ÇÄ)) Ôºù œï (Œª x ‚Üí sub q x (strategic-path (œÉf x))))
    √ó
      ((x : X) ‚Üí is-sgpe {Xf x} (œïf x) (sub q x) (œÉf x))

\end{code}

In the above definition:

 * If the game tree is empty, then the strategy is empty, and we say
   that it is true that it is in sgpe, where "true" is represented by
   the unit type ùüô in propositions-as-types.

 * If the game tree has a root X followed by a forest Xf, then the
   strategy must be of the form x‚ÇÄ :: œÉf, where x‚ÇÄ is the first move
   according to the strategy, and where œÉf is a forest of strategies
   that depends on a deviation x.

   So the first part

     sub q x‚ÇÄ (strategic-path (œÉf x‚ÇÄ)) Ôºù œï (Œª x ‚Üí sub q x (strategic-path (œÉf x)))

   of the definition is as in the comment above, but with a partial
   play of length k=0, and the second (inductive) part, says that the
   substrategy œÉf x, for any deviation x, is in subgame perfect
   equilibrium in the subgame

     (Xf x , R , œïf x , sub q x).

As discussed above, we say that a strategy for a game is optimal if it
is in subgame perfect equilibrium.

\begin{code}

is-optimal : (G : Game) (œÉ : Strategy (Xt G)) ‚Üí Type
is-optimal (game Xt œït q) œÉ = is-sgpe {Xt} q œït œÉ

\end{code}

The main lemma is that the optimal outcome is the same thing as the
application of the outcome function to the path induced by a strategy
in perfect subgame equilibrium.

The following is Theorem 3.1 of reference [1].

\begin{code}

sgpe-lemma : Fun-Ext
           ‚Üí (Xt : ùïã) (œït : ùìö Xt) (q : Path Xt ‚Üí R) (œÉ : Strategy Xt)
           ‚Üí is-sgpe œït q œÉ
           ‚Üí q (strategic-path œÉ) Ôºù K-sequence œït q
sgpe-lemma fe []       ‚ü®‚ü©        q ‚ü®‚ü©        ‚ü®‚ü©       = refl
sgpe-lemma fe (X ‚à∑ Xf) (œï :: œït) q (a :: œÉf) (h :: t) = Œ≥
 where
  observation-t : type-of t Ôºù ((x : X) ‚Üí is-sgpe (œït x) (sub q x) (œÉf x))
  observation-t = refl

  IH : (x : X) ‚Üí sub q x (strategic-path (œÉf x)) Ôºù K-sequence (œït x) (sub q x)
  IH x = sgpe-lemma fe (Xf x) (œït x) (sub q x) (œÉf x) (t x)

  Œ≥ = sub q a (strategic-path (œÉf a))           Ôºù‚ü® h ‚ü©
      œï (Œª x ‚Üí sub q x (strategic-path (œÉf x))) Ôºù‚ü® ap œï (dfunext fe IH) ‚ü©
      œï (Œª x ‚Üí K-sequence (œït x) (sub q x))     ‚àé

\end{code}

This can be reformulated as follows in terms of the type of games:

\begin{code}

optimality-theorem : Fun-Ext
                   ‚Üí (G : Game) (œÉ : Strategy (Xt G))
                   ‚Üí is-optimal G œÉ
                   ‚Üí q G (strategic-path œÉ) Ôºù optimal-outcome G
optimality-theorem fe (game Xt œït q) = sgpe-lemma fe Xt q œït

\end{code}

We now show how to use selection functions to compute a sgpe strategy.

We use selection functions, as in Section 2 of reference [1], defined
in another module.

ùìô assigns selection functions to the nodes.

\begin{code}

ùìô : ùïã ‚Üí Type
ùìô []       = ùüô
ùìô (X ‚à∑ Xf) = J X √ó ((x : X) ‚Üí ùìô (Xf x))

\end{code}

 * Œµ ranges over the type J X of selection functions.
 * Œµt ranges over the type ùìô Xt of selection-function trees.
 * Œµf ranges over the type (x : X) ‚Üí ùìô (Xf x) of selection-function forests.

Sequencing selection functions, as constructed in Definition 12 of
reference [1], but using our tree representation of games instead:

\begin{code}

J-sequence : {Xt : ùïã} ‚Üí ùìô Xt ‚Üí J (Path Xt)
J-sequence {[]}     ‚ü®‚ü©        = Œª q ‚Üí ‚ü®‚ü©
J-sequence {X ‚à∑ Xf} (Œµ :: Œµf) = Œµ ‚äó·¥∂ (Œª x ‚Üí J-sequence {Xf x} (Œµf x))

\end{code}

The following, which defines a strategy from given selection
functions, is defined in Theorem 5.4 of [1], with the difference that
here, for the moment, we consider only single-valued quantifiers.

\begin{code}

selection-strategy : {Xt : ùïã} ‚Üí ùìô Xt ‚Üí (Path Xt ‚Üí R) ‚Üí Strategy Xt
selection-strategy {[]}     ‚ü®‚ü©           q = ‚ü®‚ü©
selection-strategy {X ‚à∑ Xf} Œµt@(Œµ :: Œµf) q = x‚ÇÄ :: œÉf
 where
  x‚ÇÄ : X
  x‚ÇÄ = path-head (J-sequence Œµt q)

  œÉf : (x : X) ‚Üí Strategy (Xf x)
  œÉf x = selection-strategy {Xf x} (Œµf x) (sub q x)

\end{code}

We convert a selection function into a quantifier as in Definition 10
of [1], using the function overline, defined in another module.

The work with the definition of a selection function being a selection
function for a quantifier as in Section 1 on [1], defined in another
module.

We generalize it to selection-function and quantifier trees in the
obvious way, by induction:

\begin{code}

open JK R

_are-selections-of_ : {Xt : ùïã} ‚Üí ùìô Xt ‚Üí ùìö Xt ‚Üí Type
_are-selections-of_ {[]}     ‚ü®‚ü©        ‚ü®‚ü©        = ùüô
_are-selections-of_ {X ‚à∑ Xf} (Œµ :: Œµf) (œï :: œïf) = (Œµ is-a-selection-of œï)
                                                 √ó ((x : X) ‚Üí (Œµf x) are-selections-of (œïf x))

\end{code}

The following is the application of overline to each selection
function of a tree:

\begin{code}

Overline : {Xt : ùïã} ‚Üí ùìô Xt ‚Üí ùìö Xt
Overline {[]}     ‚ü®‚ü©        = ‚ü®‚ü©
Overline {X ‚à∑ Xf} (Œµ :: Œµs) = overline Œµ :: (Œª x ‚Üí Overline {Xf x} (Œµs x))

\end{code}

The following is proved by straightforward induction on trees:

\begin{code}

observation : Fun-Ext
            ‚Üí {Xt : ùïã} (Œµt : ùìô Xt) (œït : ùìö Xt)
            ‚Üí Œµt are-selections-of œït
            ‚Üí Overline Œµt Ôºù œït
observation fe {[]}     ‚ü®‚ü©        ‚ü®‚ü©        ‚ü®‚ü©        = refl
observation fe {X ‚à∑ Xf} (Œµ :: Œµf) (œï :: œïf) (a :: af) = Œ≥
 where
  IH : (x : X) ‚Üí Overline (Œµf x) Ôºù œïf x
  IH x = observation fe {Xf x} (Œµf x) (œïf x) (af x)

  I : overline Œµ Ôºù œï
  I = dfunext fe a

  II : (Œª x ‚Üí Overline (Œµf x)) Ôºù œïf
  II = dfunext fe IH

  Œ≥ : overline Œµ :: (Œª x ‚Üí Overline (Œµf x)) Ôºù œï :: œïf
  Œ≥ = ap‚ÇÇ _::_ I II

\end{code}

Notice that the converse is also true, that is, if Overline Œµt Ôºù œït
then Œµt are selections of œït, but we don't need this fact here.

\begin{code}

main-lemma : {Xt : ùïã} (Œµt : ùìô Xt) (q : Path Xt ‚Üí R)
           ‚Üí strategic-path (selection-strategy Œµt q)
           Ôºù J-sequence Œµt q
main-lemma {[]}     ‚ü®‚ü©           q = refl
main-lemma {X ‚à∑ Xf} Œµt@(Œµ :: Œµf) q =
 strategic-path (selection-strategy (Œµ :: Œµf) q) Ôºù‚ü® refl ‚ü©
 x‚ÇÄ :: strategic-path (œÉf x‚ÇÄ)                    Ôºù‚ü® ap (x‚ÇÄ ::_) IH ‚ü©
 x‚ÇÄ :: J-sequence {Xf x‚ÇÄ} (Œµf x‚ÇÄ) (sub q x‚ÇÄ)     Ôºù‚ü® refl ‚ü©
 x‚ÇÄ :: ŒΩ x‚ÇÄ                                      Ôºù‚ü® refl ‚ü©
 (Œµ ‚äó·¥∂ (Œª x ‚Üí J-sequence {Xf x} (Œµf x))) q       Ôºù‚ü® refl ‚ü©
 J-sequence (Œµ :: Œµf) q                          ‚àé
 where
  ŒΩ : (x : X) ‚Üí Path (Xf x)
  ŒΩ x = J-sequence {Xf x} (Œµf x) (sub q x)

  x‚ÇÄ : X
  x‚ÇÄ = Œµ (Œª x ‚Üí sub q x (ŒΩ x))

  œÉf : (x : X) ‚Üí Strategy (Xf x)
  œÉf x = selection-strategy {Xf x} (Œµf x) (sub q x)

  IH : strategic-path (œÉf x‚ÇÄ) Ôºù J-sequence {Xf x‚ÇÄ} (Œµf x‚ÇÄ) (sub q x‚ÇÄ)
  IH = main-lemma (Œµf x‚ÇÄ) (sub q x‚ÇÄ)

selection-strategy-lemma : Fun-Ext
                         ‚Üí {Xt : ùïã} (Œµt : ùìô Xt) (q : Path Xt ‚Üí R)
                         ‚Üí is-sgpe (Overline Œµt) q (selection-strategy Œµt q)
selection-strategy-lemma fe {[]}     ‚ü®‚ü©           q = ‚ü®‚ü©
selection-strategy-lemma fe {X ‚à∑ Xf} Œµt@(Œµ :: Œµf) q = Œ≥
 where
  œÉf : (x : X) ‚Üí Strategy (Xf x)
  œÉf x = selection-strategy (Œµf x) (sub q x)

  x‚ÇÄ x‚ÇÅ : X
  x‚ÇÄ = Œµ (Œª x ‚Üí sub q x (J-sequence (Œµf x) (sub q x)))
  x‚ÇÅ = Œµ (Œª x ‚Üí sub q x (strategic-path (œÉf x)))

  I : (x : X) ‚Üí strategic-path (œÉf x) Ôºù J-sequence (Œµf x) (sub q x)
  I x = main-lemma (Œµf x) (sub q x)

  II : x‚ÇÅ Ôºù x‚ÇÄ
  II = ap (Œª - ‚Üí Œµ (Œª x ‚Üí sub q x (- x))) (dfunext fe I)

  III = overline Œµ (Œª x ‚Üí sub q x (strategic-path (œÉf x))) Ôºù‚ü® refl ‚ü©
        sub q x‚ÇÅ (strategic-path (œÉf x‚ÇÅ))                  Ôºù‚ü® IV ‚ü©
        sub q x‚ÇÄ (strategic-path (œÉf x‚ÇÄ))                  ‚àé

   where
    IV = ap (Œª - ‚Üí sub q - (strategic-path (œÉf -))) II

  IH : (x : X) ‚Üí is-sgpe
                   (Overline (Œµf x))
                   (sub q x)
                   (selection-strategy (Œµf x) (sub q x))
  IH x = selection-strategy-lemma fe (Œµf x) (sub q x)

  Œ≥ : is-sgpe (Overline Œµt) q (x‚ÇÄ :: œÉf)
  Œ≥ = (III ‚Åª¬π) :: IH

\end{code}

The following, which shows how to use selection functions to compute
optimal strategies, corresponds to Theorem 6.2 of [1].

\begin{code}

selection-strategy-theorem : Fun-Ext
                           ‚Üí {Xt : ùïã} (Œµt : ùìô Xt)
                             (œït : ùìö Xt) (q : Path Xt ‚Üí R)
                           ‚Üí Œµt are-selections-of œït
                           ‚Üí is-sgpe œït q (selection-strategy Œµt q)
selection-strategy-theorem fe Œµt œït q a = III
 where
  I : Overline Œµt Ôºù œït
  I = observation fe Œµt œït a

  II : is-sgpe (Overline Œµt) q (selection-strategy Œµt q)
  II = selection-strategy-lemma fe Œµt q

  III : is-sgpe œït q (selection-strategy Œµt q)
  III = transport (Œª - ‚Üí is-sgpe - q (selection-strategy Œµt q)) I II


Selection-Strategy-Theorem : Fun-Ext
                           ‚Üí (G : Game) (Œµt : ùìô (Xt G))
                           ‚Üí Œµt are-selections-of (œït G)
                           ‚Üí is-optimal G (selection-strategy Œµt (q G))
Selection-Strategy-Theorem fe (game Xt œït q) Œµt = selection-strategy-theorem fe Œµt q œït

\end{code}

Added 27th August 2023 after the above was submitted for publication.

\begin{code}

selection-strategy-corollary : Fun-Ext
                             ‚Üí (G : Game) (Œµt : ùìô (Xt G))
                             ‚Üí Œµt are-selections-of (œït G)
                             ‚Üí q G (J-sequence Œµt (q G)) Ôºù optimal-outcome G
selection-strategy-corollary fe G Œµt a =
 q G (J-sequence Œµt (q G))                          Ôºù‚ü® I ‚ü©
 q G (strategic-path (selection-strategy Œµt (q G))) Ôºù‚ü® II ‚ü©
 optimal-outcome G                                  ‚àé
  where
   I  = ap (q G) ((main-lemma Œµt (q G))‚Åª¬π)
   II = sgpe-lemma fe (Xt G) (œït G) (q G)
         (selection-strategy Œµt (q G))
         (Selection-Strategy-Theorem fe G Œµt a)

\end{code}
